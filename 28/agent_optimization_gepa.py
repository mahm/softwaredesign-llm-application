# -*- coding: utf-8 -*-
"""
File Exploration Agent Optimization Script (GEPA)

This script optimizes the file exploration agent using GEPA (Genetic-Pareto) optimization.
"""

import os
import sys
import random
import argparse
import logging
import dspy
from datetime import datetime

from config import configure_lm, SMART_MODEL, FAST_MODEL
from agent_module import FileExplorationAgent
from dataset_loader import load_file_exploration_dataset

# Optimized model save path (symlink to latest)
GEPA_OPTIMIZED_MODEL_LATEST = "artifact/agent_gepa_optimized_latest.json"


class Tee:
    """Class to output to both console and file"""

    def __init__(self, file_path: str, original_stdout):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = original_stdout

    def write(self, message: str) -> None:
        self.stdout.write(message)
        self.file.write(message)
        self.file.flush()

    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()

    def close(self) -> None:
        self.file.close()


class ReportEvaluation(dspy.Signature):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«æŽ¢ç´¢ãƒ¬ãƒãƒ¼ãƒˆã‚’è©•ä¾¡ã—ã¾ã™ã€‚

    criteriaã«è¨˜è¼‰ã•ã‚ŒãŸè©•ä¾¡åŸºæº–ã«åŽ³å¯†ã«å¾“ã£ã¦è©•ä¾¡ã—ã¾ã™ã€‚
    ã“ã®Signatureã¯è©•ä¾¡ã®ã€Œåž‹ã€ã®ã¿ã‚’å®šç¾©ã—ã€
    å…·ä½“çš„ãªè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã¯å…¨ã¦criteriaã«å§”è­²ã—ã¾ã™ã€‚
    """

    task: str = dspy.InputField(
        desc="ãƒ•ã‚¡ã‚¤ãƒ«æŽ¢ç´¢ã‚¿ã‚¹ã‚¯ã®èª¬æ˜Žã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ä¸Žãˆã‚‰ã‚ŒãŸæŒ‡ç¤ºã€‚"
    )

    report: str = dspy.InputField(
        desc="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç”Ÿæˆã—ãŸãƒ¬ãƒãƒ¼ãƒˆã®å…¨æ–‡ã€‚"
    )

    criteria: str = dspy.InputField(
        desc="""è©•ä¾¡åŸºæº–ã®å®Œå…¨ãªè¨˜è¿°ã€‚

ã“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ä»¥ä¸‹ãŒå®Œå…¨ã«æ˜Žç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼š
- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ–¹æ³•ï¼ˆ0-10ç‚¹ã®é…åˆ†ï¼‰
- å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨ãƒªã‚¹ãƒˆï¼ˆconfig.py, rag_optimization_gepa.pyãªã©ï¼‰
- ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨ãƒªã‚¹ãƒˆï¼ˆREADME.md: +0.5ç‚¹ãªã©ï¼‰
- å¿…é ˆè¦ç´ ã®å®Œå…¨ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ã®é…ç‚¹å«ã‚€ï¼‰
- æƒ…å ±çµ±åˆã®è©•ä¾¡åŸºæº–

æ›–æ˜§ãªè¡¨ç¾ï¼ˆã€Œç­‰ã€ã€Œãªã©ã€ã€Œä¸»è¦ãªã€ï¼‰ã¯ä¸€åˆ‡å«ã¾ã‚Œã¾ã›ã‚“ã€‚
ã“ã®criteriaã«è¨˜è¼‰ã•ã‚ŒãŸåŸºæº–ã«åŽ³å¯†ã«å¾“ã£ã¦ãã ã•ã„ã€‚"""
    )

    score: int = dspy.OutputField(
        desc="criteriaã«åŸºã¥ã„ã¦ç®—å‡ºã•ã‚ŒãŸç·åˆã‚¹ã‚³ã‚¢ï¼ˆ0-10ã®æ•´æ•°ï¼‰ã€‚"
    )

    explanation: str = dspy.OutputField(
        desc="""è©•ä¾¡ç†ç”±ã®è©³ç´°ï¼ˆ200-400æ–‡å­—ï¼‰ã€‚

ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šè©•ä¾¡: ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã ã‹ã€criteriaã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ç…§åˆ
2. å¿…é ˆè¦ç´ è©•ä¾¡: criteriaã®å¿…é ˆè¦ç´ ãƒªã‚¹ãƒˆã¨ç…§åˆã€å«ã¾ã‚ŒãŸè¦ç´ /æ¬ è½è¦ç´ 
3. æƒ…å ±çµ±åˆè©•ä¾¡: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®é–¢ä¿‚æ€§èª¬æ˜Žã®è³ª
4. ã‚¹ã‚³ã‚¢å†…è¨³: å„é …ç›®ã§ä½•ç‚¹ç²å¾—ã—ãŸã‹"""
    )

    improvement_suggestions: str = dspy.OutputField(
        desc="""GEPAãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®æ”¹å–„ææ¡ˆï¼ˆ150-300æ–‡å­—ï¼‰ã€‚

å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªææ¡ˆã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ï¼š
- "taskã«ãƒ•ã‚¡ã‚¤ãƒ«åãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€ã¾ãšãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚€ã¹ã"
- "importæ–‡ã‚’è¦‹ã¤ã‘ãŸã‚‰ã€ãã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã‚€ã¹ã"
- "å¤‰æ•°å®šç¾©ã‚’è¦‹ã¤ã‘ãŸã‚‰ã€ãã®å¤‰æ•°ã®ä½¿ç”¨ç®‡æ‰€ã‚‚æŽ¢ã™ã¹ã"

æŠ½è±¡çš„ãªææ¡ˆï¼ˆã€Œã‚‚ã£ã¨è©³ã—ãã€ãªã©ï¼‰ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"""
    )


def create_llm_judge_metric(eval_lm):
    """
    LLM as a Judgeè©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ä½œæˆã—ã¾ã™ã€‚

    Args:
        eval_lm: è©•ä¾¡ç”¨LM (gpt-4.1-miniæŽ¨å¥¨)

    Returns:
        è©•ä¾¡é–¢æ•°ï¼ˆgold, pred, trace=None â†’ float scoreï¼‰
    """
    evaluator = dspy.ChainOfThought(ReportEvaluation)

    def llm_judge_metric(gold, pred, trace=None):
        """
        LLM as a Judgeã«ã‚ˆã‚‹è©•ä¾¡ã€‚

        Args:
            gold: Goldæ¨™æº–ãƒ‡ãƒ¼ã‚¿ï¼ˆcriteriaã‚’å«ã‚€ï¼‰
            pred: äºˆæ¸¬çµæžœï¼ˆreportã‚’å«ã‚€ï¼‰
            trace: å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆoptionalï¼‰

        Returns:
            float: 0.0-1.0ã®ã‚¹ã‚³ã‚¢
        """
        # ãƒ¬ãƒãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0ç‚¹
        if not hasattr(pred, 'report') or not pred.report:
            return 0.0

        # criteriaãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if not hasattr(gold, 'criteria') or not gold.criteria:
            raise ValueError("Gold example must have 'criteria' field for LLM as a Judge evaluation")

        # LLM as a Judgeã§è©•ä¾¡
        with dspy.context(lm=eval_lm):
            eval_result = evaluator(
                task=gold.task,
                report=pred.report,
                criteria=gold.criteria
            )

        # ã‚¹ã‚³ã‚¢ã‚’0-10ã‹ã‚‰0-1ã«æ­£è¦åŒ–
        raw_score = eval_result.score
        try:
            score = float(raw_score)
            score = min(10.0, max(0.0, score)) / 10.0
        except (ValueError, TypeError):
            score = 0.0

        return score

    return llm_judge_metric


def create_gepa_llm_judge_metric(eval_lm):
    """
    GEPAæœ€é©åŒ–ç”¨ã®LLM as a Judgeè©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ä½œæˆã—ã¾ã™ã€‚

    GEPAãŒè¦æ±‚ã™ã‚‹ScoreWithFeedbackå½¢å¼ï¼ˆscore + feedback + improvement_suggestionsï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

    Args:
        eval_lm: è©•ä¾¡ç”¨LM (gpt-4.1-miniæŽ¨å¥¨)

    Returns:
        è©•ä¾¡é–¢æ•°ï¼ˆgold, pred, trace=None, pred_name=None, pred_trace=None â†’ dspy.Predictionï¼‰
    """
    evaluator = dspy.ChainOfThought(ReportEvaluation)

    def gepa_llm_judge_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):
        """
        GEPAç”¨LLM as a Judgeè©•ä¾¡ã€‚

        Args:
            gold: Goldæ¨™æº–ãƒ‡ãƒ¼ã‚¿ï¼ˆcriteriaã‚’å«ã‚€ï¼‰
            pred: äºˆæ¸¬çµæžœï¼ˆreportã‚’å«ã‚€ï¼‰
            trace: Program execution trace (optional)
            pred_name: Name of specific predictor being optimized (optional)
            pred_trace: Execution trace of specific predictor (optional)

        Returns:
            dspy.Prediction: ScoreWithFeedback type (score, feedback, improvement_suggestions)
        """
        # ãƒ¬ãƒãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not hasattr(pred, 'report') or not pred.report:
            return dspy.Prediction(
                score=0.0,
                feedback="âŒ No report generated",
                improvement_suggestions="ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€ls_directoryã€read_fileã€write_fileãƒ„ãƒ¼ãƒ«ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
            )

        # criteriaãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if not hasattr(gold, 'criteria') or not gold.criteria:
            raise ValueError("Gold example must have 'criteria' field for LLM as a Judge evaluation")

        # LLM as a Judgeã§è©•ä¾¡
        with dspy.context(lm=eval_lm):
            eval_result = evaluator(
                task=gold.task,
                report=pred.report,
                criteria=gold.criteria
            )

        # ã‚¹ã‚³ã‚¢ã‚’0-10ã‹ã‚‰0-1ã«æ­£è¦åŒ–
        raw_score = eval_result.score
        try:
            score = float(raw_score)
            score = min(10.0, max(0.0, score)) / 10.0
        except (ValueError, TypeError):
            score = 0.0

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆexplanationã‹ã‚‰ç°¡æ½”ç‰ˆã‚’ç”Ÿæˆï¼‰
        feedback = f"Score: {raw_score}/10"
        if hasattr(eval_result, 'explanation') and eval_result.explanation:
            # explanationã®æœ€åˆã®100æ–‡å­—ã‚’æŠ½å‡º
            explanation_short = eval_result.explanation[:100]
            if len(eval_result.explanation) > 100:
                explanation_short += "..."
            feedback += f" | {explanation_short}"

        # æ”¹å–„ææ¡ˆï¼ˆGEPA reflectionç”¨ï¼‰
        improvement_suggestions = ""
        if hasattr(eval_result, 'improvement_suggestions') and eval_result.improvement_suggestions:
            improvement_suggestions = eval_result.improvement_suggestions

        # predictoråã‚’è¿½åŠ 
        if pred_name:
            feedback += f" | [{pred_name}]"

        # GEPA expects ScoreWithFeedback type
        return dspy.Prediction(
            score=score,
            feedback=feedback,
            improvement_suggestions=improvement_suggestions,
            # è©³ç´°æƒ…å ±ã‚’ä¿æŒï¼ˆãƒ­ã‚°ç”¨ï¼‰
            explanation=eval_result.explanation if hasattr(eval_result, 'explanation') else "",
            raw_score=raw_score
        )

    return gepa_llm_judge_metric


def gepa_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):
    """
    GEPA metric function with score + textual feedback.

    Args:
        gold: Gold standard data
        pred: Prediction result
        trace: Program execution trace (optional)
        pred_name: Name of specific predictor being optimized (optional)
        pred_trace: Execution trace of specific predictor (optional)

    Returns:
        dspy.Prediction: ScoreWithFeedback type (with score and feedback fields)
    """
    # Calculate base score
    score = file_exploration_metric(gold, pred, trace)

    # Generate feedback
    feedback_parts = []

    # Evaluate report quality
    if not hasattr(pred, 'report') or not pred.report:
        feedback_parts.append(" No report generated")
    else:
        report = pred.report.strip()
        report_len = len(report)

        if report_len >= 100:
            feedback_parts.append(f" Substantial report ({report_len} chars)")
        else:
            feedback_parts.append(f"ï¿½ Short report ({report_len} chars)")

        # Check for error messages
        if any(err in report.lower() for err in ['error:', 'failed', 'could not']):
            feedback_parts.append("ï¿½ Contains error messages")

        # Check structure
        lines = len(report.split('\n'))
        if lines >= 5:
            feedback_parts.append(f" Well-structured ({lines} lines)")
        elif lines >= 2:
            feedback_parts.append(f"ï¿½ Basic structure ({lines} lines)")

    # Task-specific feedback
    if hasattr(gold, 'task'):
        task_lower = gold.task.lower()
        if 'list' in task_lower or 'find' in task_lower:
            if hasattr(pred, 'report') and pred.report:
                # Check if report contains file listings
                if any(ext in pred.report for ext in ['.py', '.json', '.md', '.txt', 'FILE', 'DIR']):
                    feedback_parts.append(" Contains file information")

    # Add predictor name if specified
    if pred_name:
        feedback_parts.append(f"[{pred_name}]")

    # Generate feedback string
    feedback = " | ".join(feedback_parts)

    # GEPA expects ScoreWithFeedback type (dspy.Prediction)
    return dspy.Prediction(
        score=score,
        feedback=feedback
    )


def log_metric_evaluation(gold, pred, trace, pred_name, pred_trace, result):
    """
    Log metric evaluation.

    Args:
        gold: Gold standard data
        pred: Prediction result
        trace: Program execution trace
        pred_name: Predictor name
        pred_trace: Predictor execution trace
        result: Metric calculation result (dspy.Prediction)
    """
    logger = logging.getLogger("gepa_optimization")

    # Log input arguments
    logger.info(f"gold = {repr(gold)}")
    logger.info(f"pred = {repr(pred)}")
    logger.info(f"trace = {repr(trace) if trace is not None else 'None'}")
    logger.info(f"pred_name = {repr(pred_name) if pred_name is not None else 'None'}")
    logger.info(f"pred_trace = {repr(pred_trace) if pred_trace is not None else 'None'}")

    logger.info("-" * 80)

    # Log calculation results
    logger.info(f"score = {result.score}")
    logger.info(f"feedback = {result.feedback}")
    logger.info("=" * 80)


def gepa_metric_with_feedback_logged(gold, pred, trace=None, pred_name=None, pred_trace=None):
    """
    GEPA metric function with logging.

    Args:
        gold: Gold standard data
        pred: Prediction result
        trace: Program execution trace (optional)
        pred_name: Name of specific predictor being optimized (optional)
        pred_trace: Execution trace of specific predictor (optional)

    Returns:
        dspy.Prediction: ScoreWithFeedback type (with score and feedback fields)
    """
    result = gepa_metric_with_feedback(gold, pred, trace, pred_name, pred_trace)
    log_metric_evaluation(gold, pred, trace, pred_name, pred_trace, result)

    return result


def setup_logging(timestamp: str) -> tuple:
    """
    Setup logging environment.

    Args:
        timestamp: Timestamp string (YYYYMMDD_HHMM format)

    Returns:
        tuple: (original_stdout, tee, log_path, stdout_path)
    """
    # Create log directory
    os.makedirs("logs", exist_ok=True)

    # Log file paths
    log_filename = f"gepa_optimization_{timestamp}.log"
    log_path = os.path.join("logs", log_filename)

    stdout_filename = f"gepa_optimization_{timestamp}_stdout.log"
    stdout_path = os.path.join("logs", stdout_filename)

    # Logger configuration
    logger = logging.getLogger("gepa_optimization")
    logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    # Redirect stdout
    original_stdout = sys.stdout
    tee = Tee(stdout_path, original_stdout)
    sys.stdout = tee

    print(f"=ï¿½ Log file: {log_path}")
    print(f"=ï¿½ Stdout log: {stdout_path}")

    return original_stdout, tee, log_path, stdout_path


def cleanup_logging(original_stdout, tee: Tee, log_path: str, stdout_path: str) -> None:
    """
    Cleanup logging environment.

    Args:
        original_stdout: Original stdout
        tee: Tee object
        log_path: Log file path
        stdout_path: Stdout log path
    """
    # Cleanup logger
    logger = logging.getLogger("gepa_optimization")
    for handler in logger.handlers[:]:
        handler.close()
        logger.removeHandler(handler)

    # Restore stdout
    sys.stdout = original_stdout
    tee.close()

    # Display file paths
    print("\n Optimization complete!")
    print(f"=ï¿½ Detailed log: {log_path}")
    print(f"=ï¿½ Standard output: {stdout_path}")


def main(seed=42, dataset="train"):
    """
    Main execution function.

    Args:
        seed: Random seed (default: 42)
    """
    # Generate timestamp (shared by log and model filenames)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    # Setup logging environment
    original_stdout, tee, log_path, stdout_path = setup_logging(timestamp)

    try:
        # Load dataset
        print("=ï¿½ Loading file exploration dataset...")
        train_examples = load_file_exploration_dataset(dataset_type=dataset, random_seed=seed)
        # Note: test set is loaded separately in agent_evaluation.py
        # For GEPA optimization, we use train set only (no val set)

        # LM configuration
        print("\n=' Configuring models...")
        # GEPA reflection LM (high temperature)
        reflection_lm = configure_lm(SMART_MODEL, temperature=1.0, max_tokens=8192)
        # Inference LM (fast model)
        fast_lm = configure_lm(FAST_MODEL, temperature=0.0, max_tokens=4096)
        # Evaluation LM (for LLM as a Judge)
        from config import EVAL_MODEL
        eval_lm = configure_lm(EVAL_MODEL, temperature=0.0, max_tokens=4096)

        # Configure DSPy with default LM
        dspy.configure(lm=fast_lm)

        # Create LLM as a Judge metrics
        print("\n=\u2696\ufe0f Creating LLM as a Judge evaluation metrics...")
        llm_judge_metric = create_llm_judge_metric(eval_lm)
        gepa_llm_metric = create_gepa_llm_judge_metric(eval_lm)

        # Baseline evaluation
        print("\n=ï¿½ Evaluating baseline (train set)...")
        baseline_agent = FileExplorationAgent(max_iters=10, verbose=False)

        baseline_scores = []
        for ex in train_examples[:3]:  # Use first 3 examples for quick baseline check
            pred = baseline_agent(task=ex.task, working_directory=ex.working_directory)
            score = llm_judge_metric(ex, pred)
            baseline_scores.append(score)

        baseline_avg = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0.0
        print(f"  Baseline average score: {baseline_avg:.3f} (on {len(baseline_scores)} examples)")

        # GEPA optimization
        print("\n=ï¿½ Starting GEPA optimization...")

        # Target agent for optimization
        agent = FileExplorationAgent(max_iters=10, verbose=False)

        # GEPA configuration
        optimizer = dspy.GEPA(
            metric=gepa_llm_metric,  # LLM as a Judge metric with feedback
            auto="light",  # Optimization intensity
            reflection_lm=reflection_lm,  # LM for reflection (strong model recommended)
        )

        # Execute optimization
        optimized_agent = optimizer.compile(
            agent,
            trainset=train_examples,
        )

        # Post-optimization evaluation
        print("\n=ï¿½ Evaluating optimized agent (train set)...")
        opt_scores = []
        for ex in train_examples[:3]:  # Use first 3 examples for quick check
            pred = optimized_agent(task=ex.task, working_directory=ex.working_directory)
            score = llm_judge_metric(ex, pred)
            opt_scores.append(score)

        opt_avg = sum(opt_scores) / len(opt_scores) if opt_scores else 0.0
        print(f"  [Baseline] Avg score: {baseline_avg:.3f} (on {len(baseline_scores)} examples)")
        print(f"  [GEPA Optimized] Avg score: {opt_avg:.3f} (on {len(opt_scores)} examples)")
        print(f"  Improvement: {opt_avg - baseline_avg:+.3f}")

        # Generate filename with score (use validation score, reuse timestamp)
        score_percent = int(opt_avg * 100)
        score_str = f"score{score_percent:03d}"
        model_filename = f"agent_gepa_optimized_{timestamp}_{score_str}.json"
        model_path = os.path.join("artifact", model_filename)

        # Save model
        os.makedirs("artifact", exist_ok=True)
        optimized_agent.save(model_path)
        print(f"\n=ï¿½ Saved optimized model: {model_path}")

        # Create symlink to latest version
        if os.path.exists(GEPA_OPTIMIZED_MODEL_LATEST):
            os.remove(GEPA_OPTIMIZED_MODEL_LATEST)
        os.symlink(model_filename, GEPA_OPTIMIZED_MODEL_LATEST)

        print(f"  ï¿½ Latest link: {GEPA_OPTIMIZED_MODEL_LATEST}")

    finally:
        # Cleanup logging environment
        cleanup_logging(original_stdout, tee, log_path, stdout_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='File Exploration Agent Optimization (GEPA)')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed (default: 42)')
    parser.add_argument('--dataset', type=str, default='train',
                       choices=['train', 'mini_test'],
                       help='Dataset to use: train (10 examples) or mini_test (3 examples)')
    args = parser.parse_args()

    print(f"<1 Seed value: {args.seed}")
    print(">ï¿½ Optimization method: GEPA (Genetic-Pareto)")
    print(f"ðŸ“Š Dataset: {args.dataset}")
    main(seed=args.seed, dataset=args.dataset)
